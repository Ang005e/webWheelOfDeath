name: Sync repo to OpenAI
on: [push]

jobs:
  embed:
    runs-on: ubuntu-latest

    # ------------------------------------------------------------------
    # Shared constants
    env:
      ONE_GB_BYTES: 1073741824   # 1 GiB

    steps:
    # -------------------------------------------------------------- 0Ô∏è‚É£
    - uses: actions/checkout@v4

    # -------------------------------------------------------------- 1Ô∏è‚É£
    # Dependencies
    - name: Install OpenAI CLI + helpers
      run: |
        python -m pip install --upgrade pip
        python -m pip install --upgrade "openai>=1.75" "tiktoken>=0.5"
        sudo apt-get update -y
        sudo apt-get install -y jq

    # -------------------------------------------------------------- 2Ô∏è‚É£
    # Empty vector-store
    - name: Empty vector store
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        VECTOR_ID:       ${{ secrets.OPENAI_VECTOR_ID }}
      run: |
        python - <<'PY'
        import os, sys, time, openai
        client = openai.OpenAI()
        vs_id  = os.environ["VECTOR_ID"]

        def iter_vs_files(vs):
            page = client.vector_stores.files.list(vector_store_id=vs, limit=100, order="asc")
            while True:
                yield from page.data
                if not page.has_more:
                    break
                page = client.vector_stores.files.list(vector_store_id=vs, limit=100, order="asc", after=page.data[-1].id)

        # Delete everything
        for f in iter_vs_files(vs_id):
            client.vector_stores.files.delete(vector_store_id=vs_id, file_id=f.id)
            try:
                client.files.delete(file_id=f.id)
            except openai.NotFoundError:
                pass

        # Allow for eventual consistency
        time.sleep(2)

        if list(iter_vs_files(vs_id)):
            sys.exit("Vector-store not empty ‚Äî aborting.")
        PY

    # -------------------------------------------------------------- 3Ô∏è‚É£
    # Build dataset
    - name: Build dataset.json
      run: |
        mkdir -p upload
        python - <<'PY'
        import pathlib, subprocess, json, re, tiktoken

        MAX_TOKENS  = 2_000
        enc         = tiktoken.get_encoding("cl100k_base")
        OUT_FILE    = pathlib.Path("upload/dataset.json")

        SKIP_DIRS   = {".vs", "bin", "obj", "node_modules", "dist", "wwwroot", "packages"}
        BIN_RE      = re.compile(rb"[\x00-\x08\x0E-\x1F]")
        GEN_RE      = re.compile(r"(generated|do not edit|This code was generated by a tool)", re.I)

        EXT_LANG = {
            ".cs": "csharp", ".cshtml": "razor", ".razor": "razor",
            ".js": "javascript", ".ts": "typescript", ".html": "html",
            ".css": "css", ".json": "json", ".sql": "sql", ".md": "markdown",
        }

        def git_tracked():
            res = subprocess.check_output(
                ["git", "ls-files", "--exclude-standard", "-z"], text=True)
            return [p for p in res.split("\0") if p]

        def looks_binary(path: pathlib.Path):
            with path.open("rb") as f:
                return bool(BIN_RE.search(f.read(8192)))

        n_chunks = 0
        with OUT_FILE.open("w", encoding="utf-8") as out:
            for p in git_tracked():
                path = pathlib.Path(p)
                if any(part in SKIP_DIRS for part in path.parts):
                    continue
                if looks_binary(path):
                    continue
                try:
                    text = path.read_text("utf-8", errors="ignore")
                except Exception:
                    continue
                if GEN_RE.search(text[:2000]):
                    continue

                lang = EXT_LANG.get(path.suffix.lower(), "text")
                tokens = enc.encode(text)
                for i in range(0, len(tokens), MAX_TOKENS):
                    chunk = enc.decode(tokens[i:i+MAX_TOKENS])
                    meta  = {"path": str(path), "language": lang}
                    out.write(json.dumps({"text": chunk, "metadata": meta}) + "\n")
                    n_chunks += 1
        print(f"‚úÖ  Wrote {n_chunks} chunks ‚Üí {OUT_FILE} ({OUT_FILE.stat().st_size} bytes)")
        PY

    # -------------------------------------------------------------- 4Ô∏è‚É£
    # Upload / attach (with 1 GiB guard)
    - name: Upload / attach (with size guard)
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        VECTOR_ID:       ${{ secrets.OPENAI_VECTOR_ID }}
        ONE_GB_BYTES:    ${{ env.ONE_GB_BYTES }}
      run: |
        set -euo pipefail

        DATASET=upload/dataset.json
        SIZE=$(stat --printf="%s" "$DATASET")
        echo "üì¶ dataset.json size: $SIZE bytes"

        if [ "$SIZE" -gt "$ONE_GB_BYTES" ]; then
          echo "‚ùå dataset.json exceeds 1 GB ‚Äì cancelling upload."

          ERR_FILE=upload/error.jsonl
          printf '{"text":"ERROR: dataset.json size (%d bytes) exceeds 1GB limit. Upload cancelled.","metadata":{"type":"error","size_bytes":%d}}\n' "$SIZE" "$SIZE" > "$ERR_FILE"

          echo "üì§ Uploading error record instead‚Ä¶"
          json=$(openai api files.create -f "$ERR_FILE" --purpose assistants)
          FILE_ID=$(jq -r '.id' <<<"$json")
          echo "   ‚Üí Error File ID: $FILE_ID"
          export FILE_ID

          echo "üîó  Attaching error file to vector-store‚Ä¶"
          python - <<'PY'
          import os, openai
          client = openai.OpenAI()
          client.vector_stores.files.create(
              vector_store_id=os.environ["VECTOR_ID"],
              file_id=os.environ["FILE_ID"])
          print("‚úÖ  Error file attached.")
          PY
          exit 1
        fi
          
        echo "üì§  Uploading dataset.json‚Ä¶"
        json=$(openai api files.create -f "$DATASET" --purpose assistants)
        FILE_ID=$(jq -r '.id' <<<"$json")
        echo "   ‚Üí File ID: $FILE_ID"
        export FILE_ID

        echo "üîó  Attaching to vector-store‚Ä¶"
        python - <<'PY'
        import os, openai
        client = openai.OpenAI()
        client.vector_stores.files.create(
            vector_store_id=os.environ["VECTOR_ID"],
            file_id=os.environ["FILE_ID"])
        print("‚úÖ  Dataset attached.")
        PY
