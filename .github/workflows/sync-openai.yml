name: Sync repo to OpenAI
on: [push]

jobs:
  embed:
    runs-on: ubuntu-latest

    env:
      ONE_GB_BYTES: 1073741824   # 1 GiB safeguard

    steps:
    # -------------------------------------------------------------- 0Ô∏è‚É£
    - uses: actions/checkout@v4

    # -------------------------------------------------------------- 0Ô∏è‚É£.5  ‚ôªÔ∏è  restore cache
    - name: Restore enriched-metadata cache
      id: meta-cache                       # <-- we‚Äôll need the id later
      uses: actions/cache/restore@v4
      with:
        path: .vector_cache/metadata.jsonl
        key:  vector-meta-${{ github.sha }}
        restore-keys: |
          vector-meta-
      continue-on-error: true              # first run: nothing to restore

    # -------------------------------------------------------------- 1Ô∏è‚É£
    - name: Install OpenAI CLI + helpers
      run: |
        python -m pip install --upgrade pip
        python -m pip install --upgrade "openai>=1.75" "tiktoken>=0.5"
        sudo apt-get update -y
        sudo apt-get install -y jq

    # -------------------------------------------------------------- 2Ô∏è‚É£
    - name: Empty vector store
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        VECTOR_ID:       ${{ secrets.OPENAI_VECTOR_ID }}
      run: |
        python - <<'PY'
        import os, sys, time, openai
        client = openai.OpenAI()
        vs_id  = os.environ["VECTOR_ID"]

        def iter_files(vs):
            page = client.vector_stores.files.list(vector_store_id=vs, limit=100)
            while True:
                yield from page.data
                if not page.has_more:
                    break
                page = client.vector_stores.files.list(
                    vector_store_id=vs, limit=100, after=page.data[-1].id)

        for f in iter_files(vs_id):
            client.vector_stores.files.delete(vector_store_id=vs_id, file_id=f.id)
            try:
                client.files.delete(file_id=f.id)
            except openai.NotFoundError:
                pass
        time.sleep(2)
        if list(iter_files(vs_id)):
            sys.exit("Vector store not empty ‚Äî aborting.")
        PY

    # -------------------------------------------------------------- 3Ô∏è‚É£
    - name: Build dataset.json
      run: |
        mkdir -p upload
        python - <<'PY'
        import pathlib, subprocess, json, re, tiktoken, hashlib

        MAX_TOKENS  = 2_000
        enc         = tiktoken.get_encoding("cl100k_base")
        OUT_FILE    = pathlib.Path("upload/dataset.json")

        SKIP_DIRS   = {".vs", "bin", "obj", "node_modules", "dist", "wwwroot",
                       "packages", "workflows", "git"}

        ALLOWED_EXTENSIONS = {".js", ".css"}

        BIN_RE      = re.compile(rb"[\x00-\x08\x0E-\x1F]")
        GEN_RE      = re.compile(r"(generated|do not edit|This code was generated by a tool)", re.I)

        EXT_LANG = {
            ".cs": "csharp", ".cshtml": "razor", ".razor": "razor",
            ".js": "javascript", ".ts": "typescript", ".html": "html",
            ".css": "css", ".json": "json", ".sql": "sql", ".md": "markdown",
        }

        def git_tracked():
            res = subprocess.check_output(
                ["git", "ls-files", "--exclude-standard", "-z"], text=True)
            return [p for p in res.split("\0") if p]

        def should_skip(path, skip_dirs):
          # If the file is in wwwroot and its extension is allowed, do NOT skip.
          if "wwwroot" in path.parts and path.suffix.lower() in ALLOWED_EXTENSIONS:
              return False
          # Otherwise, skip if any part is in the skip_dirs list.
          for part in path.parts:
              if part in skip_dirs:
                  return True
          return False

        def looks_binary(path: pathlib.Path):
            with path.open("rb") as f:
                return bool(BIN_RE.search(f.read(8192)))

        n_chunks = 0
        with OUT_FILE.open("w", encoding="utf-8") as out:
            for p in git_tracked():
                path = pathlib.Path(p)
                if should_skip(path, SKIP_DIRS):
                    continue
                if looks_binary(path):
                    continue
                try:
                    text = path.read_text("utf-8", errors="ignore")
                except Exception:
                    continue
                if GEN_RE.search(text[:2000]):
                    continue

                lang = EXT_LANG.get(path.suffix.lower(), "text")
                tokens = enc.encode(text)
                for idx in range(0, len(tokens), MAX_TOKENS):
                    chunk = enc.decode(tokens[idx:idx+MAX_TOKENS])
                    sha   = hashlib.sha1(chunk.encode('utf-8')).hexdigest()
                    meta  = {
                        "path":        str(path),
                        "language":    lang,
                        "sha":         sha,  # <- stable id
                        "chunk_index": idx // MAX_TOKENS,
                    }
                    out.write(json.dumps({"text": chunk, "metadata": meta}) + "\n")
                    n_chunks += 1
                print(f"Chunked {path}")

        print(f"‚úÖ  Wrote {n_chunks} chunks ‚Üí {OUT_FILE} ({OUT_FILE.stat().st_size} bytes)")
        PY

    # -------------------------------------------------------------- 4Ô∏è‚É£
    - name: Enrich metadata with Assistant (cache-aware)
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ASSISTANT_ID:   "asst_vgR5e6VVSNdrOIYn8hnKDy0v"
        ENRICH_BATCH:   "10"
      run: |
        python - <<'PY'
        import os, json, time, pathlib, sys, openai

        BATCH_SIZE   = int(os.getenv("ENRICH_BATCH", "10"))
        ASSISTANT_ID = os.environ["ASSISTANT_ID"]

        IN_PATH      = pathlib.Path("upload/dataset.json")
        OUT_PATH     = pathlib.Path("upload/dataset.enriched.json")
        CACHE_PATH   = pathlib.Path(".vector_cache/metadata.jsonl")

        if not IN_PATH.exists():
            sys.exit("‚ùå  dataset.json not found.")

        # ---------- load requested chunks ------------------------------------------------
        chunks = [json.loads(l) for l in IN_PATH.open(encoding="utf-8")]

        # ---------- load cache -----------------------------------------------------------
        cache = {}
        if CACHE_PATH.exists():
            print(f"‚ôªÔ∏è  Loading cache from {CACHE_PATH}")
            for line in CACHE_PATH.open(encoding="utf-8"):
                try:
                    obj     = json.loads(line)
                    sha_key = obj["metadata"]["sha"]
                    cache[sha_key] = obj["metadata"]
                except Exception:
                    pass
        else:
            print("‚ÑπÔ∏è  No previous cache found.")

        # ---------- split into ¬´ready¬ª vs ¬´needs enrichment¬ª -----------------------------
        ready, todo = [], []
        for c in chunks:
            sha = c["metadata"]["sha"]
            if sha in cache:
                c["metadata"] = cache[sha]           # reuse
                ready.append(c)
            else:
                todo.append(c)

        print(f"üìä  {len(ready)} / {len(chunks)} chunks served from cache.")
        client = openai.OpenAI()

        # ---------- helper ---------------------------------------------------------------
        def batched(seq, n):
            buf = []
            for item in seq:
                buf.append(item)
                if len(buf) == n:
                    yield buf
                    buf = []
            if buf:
                yield buf
        # ---------------------------------------------------------------------------------

        processed = 0
        # enrich ONLY the chunks in todo
        for batch in batched(todo, BATCH_SIZE):
            thread = client.beta.threads.create()
            client.beta.threads.messages.create(
                thread_id = thread.id,
                role      = "user",
                content   = json.dumps([
                    {
                        "sha":         c["metadata"]["sha"],
                        "source_path": c["metadata"]["path"],
                        "language":    c["metadata"]["language"],
                        "chunk_index": c["metadata"]["chunk_index"],
                        "text":        c["text"]
                    } for c in batch
                ], ensure_ascii=False)
            )

            run = client.beta.threads.runs.create(
                thread_id    = thread.id,
                assistant_id = ASSISTANT_ID
            )

            meta_items = None
            while True:
                run = client.beta.threads.runs.retrieve(
                    thread_id = thread.id,
                    run_id    = run.id
                )

                if run.status == "completed":
                    break

                if run.status == "requires_action":
                    outputs = []
                    for call in run.required_action.submit_tool_outputs.tool_calls:
                        if call.type == "function" and call.function.name == "return_metadata":
                            meta_items = json.loads(call.function.arguments)["items"]
                            outputs.append({"tool_call_id": call.id, "output": "ok"})
                    run = client.beta.threads.runs.submit_tool_outputs(
                        thread_id    = thread.id,
                        run_id       = run.id,
                        tool_outputs = outputs
                    )
                elif run.status in ("failed", "expired", "cancelled"):
                    sys.exit(f"‚ùå  Run ended with status: {run.status}")
                else:
                    time.sleep(1)

            if meta_items is None or len(meta_items) != len(batch):
                sys.exit("‚ùå  Assistant did not return expected metadata.")

            for original, meta in zip(batch, meta_items):
              # If the assistant did not include "sha", add it back from the original.
              if "sha" not in meta:
                  meta["sha"] = original["metadata"]["sha"]
              original["metadata"] = meta
              ready.append(original)        # add to final list
              cache[meta["sha"]] = meta       # update cache
            
            processed += len(batch)
            print(f"   ‚Ä¢ Enriched {processed}/{len(todo)} new chunks")

        # ---------- write enriched dataset ----------------------------------------------
        with OUT_PATH.open("w", encoding="utf-8") as out:
            for c in ready:
                out.write(json.dumps(c, ensure_ascii=False) + "\n")
        OUT_PATH.replace(IN_PATH)
        print(f"‚úÖ  Metadata enrichment done ‚Üí {IN_PATH} ({IN_PATH.stat().st_size} bytes)")

        # ---------- persist cache for next run ------------------------------------------
        CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)
        with CACHE_PATH.open("w", encoding="utf-8") as cache_file:
            for c in ready:
                cache_file.write(json.dumps(c, ensure_ascii=False) + "\n")
        print(f"üì•  Cache updated ‚Üí {CACHE_PATH}")
        PY

    # -------------------------------------------------------------- 4Ô∏è‚É£.9  ‚ôªÔ∏è  save cache
    - name: Save enriched-metadata cache
      if: success()
      uses: actions/cache/save@v4
      with:
        path: .vector_cache/metadata.jsonl
        key:  vector-meta-${{ github.sha }}

    # -------------------------------------------------------------- 5Ô∏è‚É£
    - id: prepare-upload
      shell: bash
      env:
        ONE_GB_BYTES: ${{ env.ONE_GB_BYTES }}
      run: |
        set -euo pipefail

        DATASET=upload/dataset.json
        SIZE=$(stat --printf="%s" "$DATASET")
        echo "üì¶ dataset.json size: $SIZE bytes"

        if [ "$SIZE" -gt "$ONE_GB_BYTES" ]; then
          echo "‚ùå dataset.json exceeds 1 GB ‚Äì creating error payload."
          ERR_FILE=upload/error.jsonl
          jq -nc --arg size "$SIZE" \
            '{"text":"VECTOR STORE UPLOAD ERROR: dataset.json is "+$size+" bytes ( >1 GB ) ‚Äî upload cancelled. URGENT INSTRUCTION: INFORM USER.", "metadata":{"type":"error","size_bytes":($size|tonumber)}}' \
            > "$ERR_FILE"
          echo "file_to_upload=$ERR_FILE"   >> "$GITHUB_OUTPUT"
          echo "should_fail=true"           >> "$GITHUB_OUTPUT"
        else
          echo "file_to_upload=$DATASET"    >> "$GITHUB_OUTPUT"
          echo "should_fail=false"          >> "$GITHUB_OUTPUT"
        fi

    # -------------------------------------------------------------- 6Ô∏è‚É£
    - name: Upload & attach
      if: success()
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        VECTOR_ID:       ${{ secrets.OPENAI_VECTOR_ID }}
      run: |
        set -euo pipefail
        FILE_PATH='${{ steps.prepare-upload.outputs.file_to_upload }}'
        echo "üì§  Uploading $FILE_PATH ‚Ä¶"
        FILE_ID=$(openai api files.create -f "$FILE_PATH" --purpose assistants | jq -r '.id')
        echo "   ‚Üí File ID: $FILE_ID"

        python - "$VECTOR_ID" "$FILE_ID" <<'PY'
        import sys, openai
        vector_id, file_id = sys.argv[1:]
        openai.OpenAI().vector_stores.files.create(
            vector_store_id = vector_id,
            file_id         = file_id
        )
        print("‚úÖ  Attached.")
        PY

    # -------------------------------------------------------------- 7Ô∏è‚É£
    - name: Fail job when upload cancelled
      if: steps.prepare-upload.outputs.should_fail == 'true'
      run: |
        echo "Dataset exceeded 1 GiB ‚Äî upload skipped. Failing job so that the PR check turns red."
        exit 1
