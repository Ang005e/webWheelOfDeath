name: Sync repo to OpenAI
on: [push]

jobs:
  embed:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    # ─────────────────────────────────────────────
    #  Dependencies
    - name: Install OpenAI CLI + helpers
      run: pip install --upgrade "openai>=1.75" "tiktoken>=0.5" jq

    # ─────────────────────────────────────────────
    #  0️⃣  Wipe vector-store (same as before)
    - name: Empty vector store
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        VECTOR_ID:       ${{ secrets.OPENAI_VECTOR_ID }}
      run: |
        python - <<'PY'
        import os, sys, openai
        client = openai.OpenAI()
        vs_id  = os.environ["VECTOR_ID"]

        def iter_vs_files(vs):
            page = client.vector_stores.files.list(
                vector_store_id=vs, limit=100, order="asc")
            while True:
                yield from page.data
                if not page.has_more:
                    break
                page = client.vector_stores.files.list(
                    vector_store_id=vs, limit=100,
                    order="asc", after=page.data[-1].id)

        for f in iter_vs_files(vs_id):
            client.vector_stores.files.delete(vector_store_id=vs_id, file_id=f.id)
            try:
                client.files.delete(file_id=f.id)
            except openai.NotFoundError:
                pass
        if list(iter_vs_files(vs_id)):
            sys.exit("Vector-store not empty — aborting.")
        PY

    # ─────────────────────────────────────────────
    #  1️⃣  Build dataset.json (Git-tracked sweep)
    - name: Build dataset.json
      run: |
        mkdir -p upload
        python - <<'PY'
        import pathlib, subprocess, json, re, os, tiktoken

        MAX_TOKENS  = 2_000
        enc         = tiktoken.get_encoding("cl100k_base")
        OUT_FILE    = pathlib.Path("upload/dataset.json")
        # Folders we never want
        SKIP_DIRS   = {".vs", "bin", "obj", "node_modules", "dist", "wwwroot", "packages"}
        BIN_RE      = re.compile(rb"[\x00-\x08\x0E-\x1F]")  # crude binary test
        GEN_RE      = re.compile(r"(generated|do not edit|This code was generated by a tool)", re.I)

        EXT_LANG = {
            ".cs": "csharp",
            ".cshtml": "razor",
            ".razor": "razor",
            ".js": "javascript",
            ".ts": "typescript",
            ".html": "html",
            ".css": "css",
            ".json": "json",
            ".sql": "sql",
            ".md": "markdown",
        }

        def git_tracked():
            res = subprocess.check_output(
                ["git", "ls-files", "--exclude-standard", "-z"],
                text=True).split("\0")
            return [p for p in res if p]

        def looks_binary(path: pathlib.Path):
            with path.open("rb") as f:
                return bool(BIN_RE.search(f.read(8192)))

        n_chunks = 0
        with OUT_FILE.open("w", encoding="utf-8") as out:
            for p in git_tracked():
                path = pathlib.Path(p)
                if any(part in SKIP_DIRS for part in path.parts):
                    continue
                if looks_binary(path):
                    continue
                text = path.read_text("utf-8", errors="ignore")
                if GEN_RE.search(text[:2000]):
                    continue

                lang = EXT_LANG.get(path.suffix.lower(), "text")
                tokens = enc.encode(text)
                for i in range(0, len(tokens), MAX_TOKENS):
                    chunk = enc.decode(tokens[i:i+MAX_TOKENS])
                    meta  = {"path": str(path), "language": lang}
                    out.write(json.dumps({"text": chunk, "metadata": meta}) + "\n")
                    n_chunks += 1
        print(f"✅  Wrote {n_chunks} chunks → {OUT_FILE}")
        PY

    # ─────────────────────────────────────────────
    # 2️⃣  Upload + attach (size-safe, YAML-safe)
    - name: Upload & attach (size-safe)
      env:
        OPENAI_API_KEY: '${{ secrets.OPENAI_API_KEY }}'
        VECTOR_ID:       '${{ secrets.OPENAI_VECTOR_ID }}'
      run: |
        set -e
    
        SIZE=$(stat -c%s "upload/dataset.json")
        LIMIT=$((1024*1024*1024))  # 1 GiB
    
        if [ "$SIZE" -gt "$LIMIT" ]; then
          echo "❌ dataset.json is too large (${SIZE} bytes > 1 GiB). Cancelling upload."
    
          # Build a one-line JSONL error file (no heredoc → no YAML issues)
          mkdir -p upload
          printf '%s\n' \
            "{\"text\":\"ERROR: dataset.json is ${SIZE} bytes (>1 GiB). Upload cancelled.\","\
    "\"metadata\":{\"type\":\"error\",\"size_bytes\":${SIZE}}}" \
            > upload/error.jsonl
    
          echo "📤 Uploading error.jsonl so the assistant can alert the user…"
          FILE_JSON=$(openai api files.create -f upload/error.jsonl --purpose assistants)
          FILE_ID=$(jq -r '.id' <<<"$FILE_JSON")
    
          echo "🔗 Attaching error file to vector store…"
          python - <<'PY'
    import os, openai
    client = openai.OpenAI()
    client.vector_stores.files.create(
        vector_store_id=os.environ["VECTOR_ID"],
        file_id=os.environ["FILE_ID"]
    )
    print("⚠️  Error record attached.")
    PY
          exit 1   # fail the job so it’s visible in Actions
        fi
    
        echo "📤 dataset.json is within limit (${SIZE} bytes). Uploading…"
        FILE_JSON=$(openai api files.create -f upload/dataset.json --purpose assistants)
        FILE_ID=$(jq -r '.id' <<<"$FILE_JSON")
    
        echo "🔗 Attaching to vector store…"
        python - <<'PY'
    import os, openai
    client = openai.OpenAI()
    client.vector_stores.files.create(
        vector_store_id=os.environ["VECTOR_ID"],
        file_id=os.environ["FILE_ID"]
    )
    print("✅  Attached.")
    PY

