name: Sync repo to OpenAI
on: [push]

jobs:
  embed:
    runs-on: ubuntu-latest

    env:
      ONE_GB_BYTES: 1073741824        # 1 GiB guard

    steps:
    # -------------------------------------------------------------- 0Ô∏è‚É£
    - uses: actions/checkout@v4
    
    # -------------------------------------------------------------- 1Ô∏è‚É£
    - name: Install OpenAI CLI + helpers
      run: |
        python -m pip install --upgrade pip
        python -m pip install --upgrade "openai>=1.75" "tiktoken>=0.5" "jsonschema>=4.22"
        sudo apt-get update -y
        sudo apt-get install -y jq
    
    # -------------------------------------------------------------- 2Ô∏è‚É£
    - name: Empty vector store
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        VECTOR_ID:       ${{ secrets.OPENAI_VECTOR_ID }}
      run: |
        python - <<'PY'
        import os, sys, time, openai
        client = openai.OpenAI()
        vs_id  = os.environ["VECTOR_ID"]
    
        def iter_files(vs):
            page = client.vector_stores.files.list(vector_store_id=vs, limit=100)
            while True:
                yield from page.data
                if not page.has_more:
                    break
                page = client.vector_stores.files.list(
                    vector_store_id=vs, limit=100, after=page.data[-1].id)
    
        for f in iter_files(vs_id):
            client.vector_stores.files.delete(vector_store_id=vs_id, file_id=f.id)
            try:
                client.files.delete(file_id=f.id)
            except openai.NotFoundError:
                pass
        time.sleep(2)
        if list(iter_files(vs_id)):
            sys.exit("Vector store not empty ‚Äî aborting.")
        PY
    
    # -------------------------------------------------------------- 3Ô∏è‚É£
    - name: Build dataset.json
      run: |
        mkdir -p upload
        python - <<'PY'
        import pathlib, subprocess, json, re, tiktoken
    
        MAX_TOKENS  = 2_000
        enc         = tiktoken.get_encoding("cl100k_base")
        OUT_FILE    = pathlib.Path("upload/dataset.json")
    
        SKIP_DIRS   = {".vs", "bin", "obj", "node_modules", "dist", "wwwroot", "packages"}
        BIN_RE      = re.compile(rb"[\x00-\x08\x0E-\x1F]")
        GEN_RE      = re.compile(r"(generated|do not edit|This code was generated by a tool)", re.I)
    
        EXT_LANG = {
            ".cs": "csharp", ".cshtml": "razor", ".razor": "razor",
            ".js": "javascript", ".ts": "typescript", ".html": "html",
            ".css": "css", ".json": "json", ".sql": "sql", ".md": "markdown",
        }
    
        def git_tracked():
            res = subprocess.check_output(
                ["git", "ls-files", "--exclude-standard", "-z"], text=True)
            return [p for p in res.split("\0") if p]
    
        def looks_binary(path: pathlib.Path):
            with path.open("rb") as f:
                return bool(BIN_RE.search(f.read(8192)))
    
        n_chunks = 0
        with OUT_FILE.open("w", encoding="utf-8") as out:
            for p in git_tracked():
                path = pathlib.Path(p)
                if any(part in SKIP_DIRS for part in path.parts):
                    continue
                if looks_binary(path):
                    continue
                try:
                    text = path.read_text("utf-8", errors="ignore")
                except Exception:
                    continue
                if GEN_RE.search(text[:2000]):
                    continue
    
                lang = EXT_LANG.get(path.suffix.lower(), "text")
                tokens = enc.encode(text)
                for i in range(0, len(tokens), MAX_TOKENS):
                    chunk = enc.decode(tokens[i:i+MAX_TOKENS])
                    meta  = {"path": str(path), "language": lang}   # bare-bones for now
                    out.write(json.dumps({"text": chunk, "metadata": meta}) + "\n")
                    n_chunks += 1
        print(f"‚úÖ  Wrote {n_chunks} chunks ‚Üí {OUT_FILE} ({OUT_FILE.stat().st_size} bytes)")
        PY
    
    # -------------------------------------------------------------- 4Ô∏è‚É£
    # Enrich metadata by calling a premade assistant in batches.
    - name: Enrich metadata with ChatGPT (Using Premade Assistant)
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        ENRICH_BATCH: "20"                # number of chunks per API call
        SCHEMA_PATH: metadata_schema.json # external JSON schema
      run: |
        python - <<'PY'
        import os, sys, json, pathlib
        from jsonschema import validate, ValidationError
        import openai

        # --------------------------------------------------------------------
        ASSISTANT_ID = "asst_vgR5e6VVSNdrOIYn8hnKDy0v"
        BATCH_SIZE   = int(os.getenv("ENRICH_BATCH", "20"))
        INPUT_PATH   = pathlib.Path("upload/dataset.json")
        OUTPUT_PATH  = pathlib.Path("upload/dataset.enriched.json")
        SCHEMA_PATH  = pathlib.Path(os.getenv("SCHEMA_PATH", "metadata_schema.json"))
        # --------------------------------------------------------------------

        if not SCHEMA_PATH.exists():
            sys.exit(f"‚ùå  JSON schema file '{SCHEMA_PATH}' not found.")

        schema = json.loads(SCHEMA_PATH.read_text())

        # Helper: batch iterator ------------------------------------------------
        def batched(iterable, n):
            batch = []
            for item in iterable:
                batch.append(item)
                if len(batch) == n:
                    yield batch
                    batch = []
            if batch:
                yield batch

        # Helper: extract all text from a Message object ------------------------
        def message_text(msg) -> str:
            """
            OpenAI v1.75 Message.content is a list[MessageContent].
            For each part with .type == 'text', pull .text.value.
            """
            if isinstance(msg.content, str):
                return msg.content
            text_parts = []
            for part in msg.content:
                if getattr(part, "type", None) == "text":
                    text_parts.append(part.text.value)
            return "".join(text_parts)

        # -----------------------------------------------------------------------
        # Load raw chunks
        with INPUT_PATH.open(encoding="utf-8") as f:
            chunks = [json.loads(l) for l in f]
        total = len(chunks)
        processed = 0

        client = openai.OpenAI()

        with OUTPUT_PATH.open("w", encoding="utf-8") as out:
            for batch in batched(chunks, BATCH_SIZE):

                # Build a simplified payload for the assistant
                simplified = []
                for local_idx, c in enumerate(batch):
                    simplified.append({
                        "source_path": c["metadata"].get("path", ""),
                        "language":    c["metadata"].get("language", ""),
                        "chunk_index": processed + local_idx,
                        "text":        c["text"]
                    })

                # ---- Assistants API flow --------------------------------------
                thread = client.beta.threads.create()
                client.beta.threads.messages.create(
                    thread_id = thread.id,
                    role      = "user",
                    content   = json.dumps(simplified, ensure_ascii=False)
                )

                run = client.beta.threads.runs.create_and_poll(
                    thread_id    = thread.id,
                    assistant_id = ASSISTANT_ID
                )

                if run.status != "completed":
                    sys.exit(f"‚ùå  Run did not complete successfully: {run.status}")

                # Retrieve the assistant‚Äôs answer (latest assistant message)
                messages = client.beta.threads.messages.list(
                    thread_id = thread.id,
                    order     = "desc",     # newest first
                    limit     = 1
                )
                
                # ------------------------------------------------------------------
                # 1. Collect every assistant message produced by this run
                all_msgs = client.beta.threads.messages.list(
                    thread_id = thread.id,
                    order     = "asc"        # oldest ‚Üí newest
                )
                
                assistant_text_parts = [
                    message_text(m) for m in all_msgs.data if m.role == "assistant"
                ]
                
                if not assistant_text_parts:
                    sys.exit("‚ùå  No assistant messages found after run.")
                
                full_response = "".join(assistant_text_parts)
                # ------------------------------------------------------------------
                # 2. Convert to JSON
                try:
                    meta_arr = json.loads(full_response)
                except Exception as err:
                    sys.exit(f"‚ùå  Could not parse assistant response as JSON:\n{full_response}")

                if len(meta_arr) != len(batch):
                    sys.exit("‚ùå  Assistant returned a different number of metadata objects than requested.")

                # Validate & write back
                for original, meta in zip(batch, meta_arr):
                    try:
                        validate(instance=meta, schema=schema)
                    except ValidationError as ve:
                        sys.exit(f"‚ùå  Schema validation failed:\n{ve}")
                    original["metadata"] = meta
                    out.write(json.dumps(original, ensure_ascii=False) + "\n")

                processed += len(batch)
                print(f"   ‚Ä¢ Enriched {processed}/{total} chunks")

        # Replace original dataset for downstream steps
        OUTPUT_PATH.replace(INPUT_PATH)
        print(f"‚úÖ  Metadata enrichment complete ‚Üí {INPUT_PATH} ({INPUT_PATH.stat().st_size} bytes)")
        PY
    
    # -------------------------------------------------------------- 5Ô∏è‚É£
    - id: prepare-upload
      shell: bash
      env:
        ONE_GB_BYTES: ${{ env.ONE_GB_BYTES }}
      run: |
        set -euo pipefail
    
        DATASET=upload/dataset.json
        SIZE=$(stat --printf="%s" "$DATASET")
        echo "üì¶ dataset.json size: $SIZE bytes"
    
        if [ "$SIZE" -gt "$ONE_GB_BYTES" ]; then
          echo "‚ùå dataset.json exceeds 1 GB ‚Äì creating error payload."
          ERR_FILE=upload/error.jsonl
          jq -nc --arg size "$SIZE" \
            '{"text":"VECTOR STORE UPLOAD ERROR: dataset.json is "+$size+" bytes ( >1 GB ) ‚Äî upload cancelled. URGENT INSTRUCTION: INFORM USER.", "metadata":{"type":"error","size_bytes":($size|tonumber)}}' \
            > "$ERR_FILE"
          echo "file_to_upload=$ERR_FILE"   >> "$GITHUB_OUTPUT"
          echo "should_fail=true"           >> "$GITHUB_OUTPUT"
        else
          echo "file_to_upload=$DATASET"    >> "$GITHUB_OUTPUT"
          echo "should_fail=false"          >> "$GITHUB_OUTPUT"
        fi
    
    # -------------------------------------------------------------- 6Ô∏è‚É£
    - name: Upload & attach
      if: success()
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        VECTOR_ID:       ${{ secrets.OPENAI_VECTOR_ID }}
      run: |
        set -euo pipefail
        FILE_PATH='${{ steps.prepare-upload.outputs.file_to_upload }}'
        echo "üì§  Uploading $FILE_PATH ‚Ä¶"
        FILE_ID=$(openai api files.create -f "$FILE_PATH" --purpose assistants | jq -r '.id')
        echo "   ‚Üí File ID: $FILE_ID"
    
        python - "$VECTOR_ID" "$FILE_ID" <<'PY'
        import sys, openai
        vector_id, file_id = sys.argv[1:]
        openai.OpenAI().vector_stores.files.create(
            vector_store_id=vector_id,
            file_id=file_id)
        print("‚úÖ  Attached.")
        PY
    
    # -------------------------------------------------------------- 7Ô∏è‚É£
    - name: Fail job when upload cancelled
      if: steps.prepare-upload.outputs.should_fail == 'true'
      run: |
        echo "Dataset exceeded 1 GiB ‚Äî upload skipped. Failing job so that the PR check turns red."
        exit 1
