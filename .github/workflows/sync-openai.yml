name: Sync repo to OpenAI
on: [push]

jobs:
  embed:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v4

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #  Dependencies
    - name: Install OpenAI CLI + helpers
      run: pip install --upgrade "openai>=1.75" "tiktoken>=0.5" jq

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #  0ï¸âƒ£  Wipe vector-store (same as before)
    - name: Empty vector store
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        VECTOR_ID:       ${{ secrets.OPENAI_VECTOR_ID }}
      run: |
        python - <<'PY'
        import os, sys, openai
        client = openai.OpenAI()
        vs_id  = os.environ["VECTOR_ID"]

        def iter_vs_files(vs):
            page = client.vector_stores.files.list(
                vector_store_id=vs, limit=100, order="asc")
            while True:
                yield from page.data
                if not page.has_more:
                    break
                page = client.vector_stores.files.list(
                    vector_store_id=vs, limit=100,
                    order="asc", after=page.data[-1].id)

        for f in iter_vs_files(vs_id):
            client.vector_stores.files.delete(vector_store_id=vs_id, file_id=f.id)
            try:
                client.files.delete(file_id=f.id)
            except openai.NotFoundError:
                pass
        if list(iter_vs_files(vs_id)):
            sys.exit("Vector-store not empty â€” aborting.")
        PY

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #  1ï¸âƒ£  Build dataset.json (Git-tracked sweep)
    - name: Build dataset.json
      run: |
        mkdir -p upload
        python - <<'PY'
        import pathlib, subprocess, json, re, os, tiktoken

        MAX_TOKENS  = 2_000
        enc         = tiktoken.get_encoding("cl100k_base")
        OUT_FILE    = pathlib.Path("upload/dataset.json")
        # Folders we never want
        SKIP_DIRS   = {".vs", "bin", "obj", "node_modules", "dist", "wwwroot", "packages"}
        BIN_RE      = re.compile(rb"[\x00-\x08\x0E-\x1F]")  # crude binary test
        GEN_RE      = re.compile(r"(generated|do not edit|This code was generated by a tool)", re.I)

        EXT_LANG = {
            ".cs": "csharp",
            ".cshtml": "razor",
            ".razor": "razor",
            ".js": "javascript",
            ".ts": "typescript",
            ".html": "html",
            ".css": "css",
            ".json": "json",
            ".sql": "sql",
            ".md": "markdown",
        }

        def git_tracked():
            res = subprocess.check_output(
                ["git", "ls-files", "--exclude-standard", "-z"],
                text=True).split("\0")
            return [p for p in res if p]

        def looks_binary(path: pathlib.Path):
            with path.open("rb") as f:
                return bool(BIN_RE.search(f.read(8192)))

        n_chunks = 0
        with OUT_FILE.open("w", encoding="utf-8") as out:
            for p in git_tracked():
                path = pathlib.Path(p)
                if any(part in SKIP_DIRS for part in path.parts):
                    continue
                if looks_binary(path):
                    continue
                text = path.read_text("utf-8", errors="ignore")
                if GEN_RE.search(text[:2000]):
                    continue

                lang = EXT_LANG.get(path.suffix.lower(), "text")
                tokens = enc.encode(text)
                for i in range(0, len(tokens), MAX_TOKENS):
                    chunk = enc.decode(tokens[i:i+MAX_TOKENS])
                    meta  = {"path": str(path), "language": lang}
                    out.write(json.dumps({"text": chunk, "metadata": meta}) + "\n")
                    n_chunks += 1
        print(f"âœ…  Wrote {n_chunks} chunks â†’ {OUT_FILE}")
        PY

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #  2ï¸âƒ£  Upload + attach to vector-store (unchanged)
    - name: Upload & attach
      env:
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
        VECTOR_ID:       ${{ secrets.OPENAI_VECTOR_ID }}
      run: |

        dataset = "upload/dataset.json"
        size_bytes = os.path.getsize(dataset)
        if size_bytes > 1 * 1024**3:  # 1 GiB
            sys.exit(f"âŒ {dataset} is {size_bytes/1024**3:.2f} GiB (> 1 GiB), aborting.")
        else:
            print(f"âœ… {dataset} is {size_bytes/1024**2:.1f} MiB, within limit.")
              
        set -e
        echo "ğŸ“¤  Uploading dataset.jsonâ€¦"
        json=$(openai api files.create -f upload/dataset.json --purpose assistants)
        FILE_ID=$(jq -r '.id' <<<"$json")
        echo "   â†’ File ID: $FILE_ID"

        echo "ğŸ”—  Attaching to vector storeâ€¦"
        python - <<'PY'
        import os, openai
        client = openai.OpenAI()
        client.vector_stores.files.create(
            vector_store_id=os.environ["VECTOR_ID"],
            file_id=os.environ["FILE_ID"])
        print("âœ…  Attached.")
        PY
